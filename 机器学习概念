机
器
学
习
笔
记

0. 基础知识
0.1 策略
0
1. 数据预处理
1.1 非平衡数据处理
1.1.1 过采样方法
最简单的方法是随机过采样，通过随机挑选少数类样本生成副本，原样本和副样本一起训练模型。为了提高样本的泛化能力，更复杂的重采样方法应运而生，最著名的方法之一是SMOTE（Synthetic Minority Over-Sampling Technique），它的基本思想是在相邻少数类样本之间随机生成合成，对于每个训练实例，找到它的K个邻居，原样本向量X和近邻样本向量Ni组成的线段之间，随机生成一个向量Si。如下公式所示：
S_i=X+α×(N_i-X)
    后来也产生给予SMOTE的改进算法，包括Borderline-SMOTE，Adaptive Synthetic Sampling，Safe-Level-SMOTE，以及SPIDER2重采样方法，主要用于解决采样临界区样本时产生的偏置问题。比如，Borderline-SMOTE只对靠近分界线的样本进行上采集，这样有利于用更少的合成样本解决算法偏置问题。Safe-Level-SMOTE在生成合成样本时，判断相邻样本的安全水平，只选择高安全系数的邻居生成新实例，降低过采样带来的噪音影响。
1.1.2 欠采样方法
    最初的方法是非启发式欠采样，随机挑选多数类样本，以达到平衡类分布的目标。启发式下采样采用数据清洗技术，有选择性的清除训练样本（大多数情况是多数类样本），代表性工作包括NCL（Neighborhood Cleaning Rule），OSS（One-Sided Selection），NearMiss-2。NCL使用Wilson的剪辑近邻法（ENN）思想清除多数类噪音。ENN认为如果一个对象的三个邻居中，至少有两个属于不同的类，那么就被认为是噪音。比如在两类数据集中有样本（Xi，y），如果y是多数类标签，三个最近邻居中至少两个属于少数类，则祛除Xi；如果y是少数类，三个最近邻中至少两个属于多数类，祛除所有最近邻居。OSS是Tomek links和压缩近邻法（CNN）的集成方法。Tomek links的基本思想是：有两个样本Ei和Ej，d(Ei, Ej)表示两者的距离，如果找不到另外一个样本Ei使得d(Ei, Ej)小于d(Ei, Ej)或者d(Ej, Ei)小于d(Ei, Ej)，那么(Ei, Ej)叫做一个Tomek link。构成Tomek links的多数类样本，被认为是噪音或者落在降临区，应该从训练集中消除。CNN原理是远离分界线的多数类样本，本认为是噪音或者落在临界区，应该从训练集中消除。CNN原理是原理分界线的多数类对分类模型的贡献最小，可以用来找到与数据集相一致的子集。基本思想是：以1-NN为例，存在E1属于E，集合E1中的元素能够正确分类集合E，那么E1与E一致。通过Tomek Links消除“不安全”样本和CNN相一致的数据子集，减少多数类样本数量，以保持少数类和多数类规模相同。NearMiss-2对于每个多数类样本，计算它与最远三个少数类样本的平均距离，祛除权值最小的多数类样本。
1.1.3 组合方法
组合方法在增加少数类样本的同时，减少多数类样本，以达到类平衡的目的。SMOTE+Tomek links，SMOTE+ENN，以及自动发现采样比等技术都属于此类型。SMOTE+Tomek links不同于简单过采样，构成Tomek link的少数类也会被清除。首先，用SMOTE对少数类样本进行上采样，然后识别整个样本的Tomic links，最后对构成Tomek link的样本进行清除。SMOTE + ENN 类似于Smote + Tomek links，不同的是用ENN深度清除更多的样本，任何样本（少数类或多数类）如果被三个邻居错误分类，都给予清除。自动发现样本比方法用交叉验证把训练集分为5组测试集，找到有最佳性能的采样比，以控制过采样和欠采样的数据规模。
除此之外，也有基于簇的重采样方法，目的是把同类样本进一步分成具有相同属性的簇，然后对簇内对象进行重采样。典型方法有Cluster-Based Oversampling(CBO)，Class Purity Maximization， Sampling-Based Clusting，Agglomerative Hierarchical Clustering，以及基于DBSCAN的DBSMOTE算法。
1.2 过拟合处理
2. 模型
2.1逻辑回归模型
2.1.1 逻辑斯谛分布
设X是连续随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数：
F(x)=P(X≤x)=1/(1+e^(-(x-u)/γ) )
f(x)=F^' (x)=e^(-(x-u)/γ)/(γ〖(1+e^(-(x-u)/γ))〗^2 )
式中，µ为位置参数，Y>0为形状参数。
2.1.2 二项逻辑斯谛回归模型
逻辑斯谛回归模型：
P(Y=1│x)=(exp⁡(w∙x+b))/(1+exp⁡(w∙x+b))
P(Y=0│x)=1/(1+exp⁡(w∙x+b))
这里，x∈Rn是输入，Y∈{0, 1}是输出，w∈Rn和b∈R是参数，w称为权值向量，b称为偏置，w•x为w和x的内积。
几率：指一个事件发生的概率与该事件不发生的比值。如果事件发生的概率是p，那么该事件的几率是p/(1-p)，该事件的对数几率或logit函数是
logit(p)=log p/(1-p)
2.1.3 模型参数估计
逻辑斯谛回归模型学习是，对于给定的训练数据集T={(x1,y1),(x2,y2),…,(xn,yn)}，其中，xi属于Rn，yi属于{0, 1}，可以应用极大似然估计法估计模型参数。
设：P(Y=1│x)=π(x),P(Y=0│x)=1-π(x)
似然函数为
∏_(i=1)^N▒〖〖[π(x_i )]〗^yi 〖[1-π(x_i )]〗^(1-yi) 〗
对数似然函数为：
L(w)=∑_(i=1)^N▒〖[y_i logπ(x_i )+(1-y_i )  log⁡(1-π(x_i ))]〗
=∑_(i=1)^N▒〖[y_i log π(x_i )/(1-π(x_i ) )+log⁡(1-π(x_i ))]〗
=∑_(i=1)^N▒〖[y_i (w∙x_i )-log⁡(1+exp⁡(w∙x_i )]〗
对L(w)求极大值，得到w的估计值。
2.1.4 多项逻辑斯谛回归
模型：
P(Y=k│x)=(exp⁡(w_k∙x))/(1+∑_(k=1)^k▒〖exp⁡(w_k∙x)〗),k=1,2,…,K-1
P(Y=K│x)=1/(1+∑_(k=1)^(k-1)▒〖exp⁡(w_k∙x)〗)
2.2 K-近邻模型

2.3 决策树模型

2.4 最大熵模型
最大熵模型（maximum entropy model）由最大熵原理推到实现。
2.4.1 最大熵原理
最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。
假设离散随机变量X的概率分布是P(X)，则其熵是
H(P)=-∑_x▒〖P(x)logP(x)〗
熵满足下列不等式：
0≤H(P)≤log|X|
式中，|X|是X的取值个数，当且仅当X的分布是均匀分布时右边的等号成立。也就是说，当X服从均匀分布时，熵最大。
2.4.2 最大熵模型
假设分类模型是一个条件概率分布P(Y|X), X∈X1属于Rn表示输入，Y∈Y1表示输出，X1和Y1分别是输入和输出的集合。这个模型表示的是对于给定的输入X，以条件概率P(Y|X)输出Y。
给定一个训练数据集
T={(x1, y1),(x2,y2),…,(xN, yN)}
学习的目标是
2.5 支持向量机
2.5.1 线性可分支持向量机
线性可分支持向量机：
给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为
W^*∙x+b^*=0
以及相应的分类决策函数
f(x)=sign(W^*∙x+b^*)
称为线性可分支持向量机
2.5.1.1 函数间隔
对于给定的训练数据集T和超平面（W, b），定义超平面（W， b）关于样本点（xi, yi）的函数间隔为
(γ_i ) ̂=y_i (w∙x_i+b)
定义超平面（w, b）关于训练数据集T的函数间隔为超平面（w，b）关于T中所有样本点（xi, yi）的函数间隔之最小值，即
γ ̂=min┬(i=1,…N)⁡(γ_i ) ̂ 
函数间隔可以表示分类预测的正确性及确信度。但是选择分离超平面时，只有函数间隔还不够。因为只要成比例地改变w和b，例如将他们改为2w和2b，超平面并没有改变，但函数间隔却成为原来的2倍。这一事实启示我们，可以对分离超平面的法向量w加某些约束，如规范化，||w||=1，使得间隔是确定的，这是函数间隔成为几何间隔
2.5.1.2 几何间隔
对于给定的训练数据集T和超平面（w，b），定义超平面（w，b）关于样本点（xi，yi）的几何间隔为
γ_i=y_i (w/‖w‖ ∙x_i+b/‖w‖ )
定义超平面（w，b）关于训练数据集T的几何间隔为超平面（w，b）关于T中所有样本点（xi，yi）的几何间隔之最小值，即
γ=min┬(i=1,∙∙∙,N)⁡〖γ_i 〗
函数间隔和几何间隔关系：
γ_i=(γ_i ) ̂/‖w‖ 
γ=γ ̂/‖w‖ 
2.5.1.3 间隔最大化
1. 最大间隔分离超平面
约束最优化问题：
max┬(w,b)⁡〖     γ〗
s.t.    y_i (w/‖w‖ ∙x_i+b/‖w‖ )≥γ,i=1,2,∙∙∙,N
即我们希望最大化超平面（w，b）关于训练数据集的几何间隔γ，约束条件表示的是超平面（w，b）关于每个训练样本的几何间隔至少是γ。
  考虑几何间隔和函数间隔的关系式，可将这个问题改写为
max┬(w,b)⁡〖γ ̂/‖w‖ 〗
s.t.y_i (w∙x_i+b)≥γ ̂,i=1,2,∙∙∙,N
函数间隔γ ̂的取值并不影响最优化问题的解。取γ ̂=1，最大化1/‖w‖ 和最小化1/2 ‖w‖^2是等价的，于是上述最优化问题可以写为：
min┬(w,b)⁡〖1/2〗 ‖w‖^2
s.t.y_i (w∙x_i+b)-1≥0,i=1,2,∙∙∙,N
这是一个凸二次规划问题。
2. 对偶问题
为了求解线性可分支持向量的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。这就是线性可分支持向量机的对偶算法。
（1）拉格朗日函数
引入拉格朗日乘子α_i≥0,i=1,2,∙∙∙,N，定义拉格朗日函数：
L(w,b,α)=1/2 ‖w‖^2-∑_(i=1)^N▒〖α_i y_i (w∙x_i+b)+∑_(i=1)^N▒α_i 〗
其中，α=〖(α_1,α_2,∙∙∙,α_N)〗^T为拉格朗日乘子向量
根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
max┬α⁡max┬(w,b)⁡〖L(w,b,α)〗 
所以，为了得到对偶问题的解，需要先求L(w,b,α)对w,b的极小，再求对α的极大。
（2）求min┬(w,b)⁡〖L(w,b,α)〗
将拉格朗日函数L(w,b,α)分别对w,b求偏导数并另其等于0.
∇_w L(w,b,α)=w-∑_(i=1)^N▒〖α_i y_i x_i=0〗
∇_b=L(w,b,α)=∑_(i=1)^N▒〖α_i y_i=0〗
得：
w=∑_(i=1)^N▒〖α_i y_i x_i 〗
∑_i^N▒〖α_i y_i=0〗
带入拉格朗日函数，既得
L(w,b,α)=1/2 ∑_(i=1)^N▒∑_(j=1)^N▒〖α_i α_j y_i α_j (x_i∙x_j )+∑_(i=1)^N▒α_i 〗
即
min┬(w,b)⁡〖L(w,b,α)=-1/2 ∑_(i=1)^N▒∑_(j=1)^N▒〖α_i α_j y_i y_j (x_i∙x_j )+∑_(i=1)^N▒α_i 〗〗
（3）求min┬(w,b)⁡〖L(w,b,α)〗对α的极大，即是对偶问题
min┬α⁡〖-1/2 ∑_(i=1)^N▒∑_(j=1)^N▒〖α_i α_j y_i y_j (x_i∙x_j )+∑_(i=1)^N▒α_i 〗〗
s.t.∑_(i=1)^N▒〖α_i y_i=0〗
α_i≥0,i=1,2,∙∙∙,N
2.5.1.4 支持向量
2.5.2 线性支持向量机
2.6 感知机模型
2.7 条件随机场
条件随机场（conditional random field, CRF）是给定一组输入随机变量条件下，另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马儿可夫随机场，条件随机场可以用于不同的预测模型。
条件随机场试图对多个变量在给定观测值后的条件概率进行建模。具体来说，若令X = {x1, x2, …, xn} 为观测序列，y = { y1, y2, …, yn} 为与之相应的标记序列，则条件随机场的目标是构建条件概率模型P(y | X)。
2.8 隐马可夫模型
2.8.1 基本概念
1、隐马尔可夫模型
隐马可夫模型是关于时序的概率模型，描述有一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马儿可夫链随机生成的状态的序列，称为状态序列；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列。序列的每一个位置又可以看作是一个时刻。
设Q是所有可能的状态几何，V是所有可能的观测的集合。
Q={q_1,q_2,∙∙∙,q_N },V={v_1 〖,v〗_2 〖,∙∙∙,v〗_M}
其中，N是可能的状态数，M是可能的观测数。
I是长度为T的状态序列，O是对应的观测序列。
I=(i1,i2,…,iT),   O=(o1,o2,…,oT)
A是状态转移概率矩阵：
A=[a_ij ]_(N×N)
其中，
a_ij=P(i_(t+1)=q_j│i_t=q_i ),    i=1,2,∙∙∙,N;  j=1,2,∙∙∙,N
是在时刻t处于状态qi的条件下在时刻t+1转移到状态qj的概率。
B是观测概率矩阵：
B=[b_j (k)]_(N×M)
其中，
bj(k)=P(ot=vk|it = qj), k = 1, 2,…,M；j = 1, 2,…,N
是在时刻t处于状态qj的条件下生成观测vk的概率。
π是初始状态概率向量：
π=（π_i）
其中，
π_i=P(i_1=q_i ),i=1,2,∙∙∙,N
是时刻t=1处于状态qi的概率。
隐马尔可夫模型由初始状态概率向量π、状态转移概率矩阵A和观测概率矩阵B决定。π和A决定状态序列，B决定观测序列。因此，隐马尔可夫模型λ可以用三元符号表示，即
λ=（A,B,π）
A,B,π称为隐马尔可夫模型的三要素。
状态转移概率矩阵A与初始状态概率矩阵向量π确定了隐藏的马尔可夫链，生成不可观测的状态序列。观测概率矩阵B确定如何从状态生成观测，与状态序列综合确定了如何产生观测序列。
从定义可知，隐马尔可夫模型作了两个基本假设：
（1）齐次马儿可夫性假设，即假设隐藏的马儿可夫链在任意时刻t的状态只依赖与前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关。
（2）观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马儿可夫链的状态，与其他观测及状态无关。
2、观察序列的生成过程
根据隐马尔可夫模型定义，可以将一个长度为T的观测序列O = (o1,o2,…,oT)的生成过程描述如下：
算法
输入：隐马尔可夫模型λ=（A,B,π），观测序列长度T；
输出：观测序列O = (o1,o2,…,oT)
（1）按照初始状态分布π产生状态i1
（2）另t = 1
（3）按照状态it的观察概率分布b_(i_t ) (k)生成O_t
（4）按照状态i_t的状态转移概率分布{a_(i_t i_(t+1) )}产生状态i_(t+1)，i_(t=1)=1,2,∙∙∙,N
（5）另t = t+1； 如果t < T，转步（3）；否则，终止
3、隐马尔可夫模型的3个基本问题
（1）概率计算问题
给定模型λ=（A, B, π）和观测序列O = (o1,o2,…,oT)，计算在模型λ下观测序列O出现的概率P(O|λ)。
（2）学习问题
已知观测序列O = (o1,o2,…,oT)，估计模型λ=（A,B,π）参数，使得在该模型下观测序列P(O|λ)最大。即用极大似然参数估计的方法估计参数。
（3）预测问题
    也称为解码问题。已知模型λ=（A，B,π）和观察序列O = (o1,o2,…,oT)，求给定观测序列条件概率P(I|O)最大的状态序列I=(i1, i2,…,iT)。即给定观测序列，求最有可能的对应的状态序列。
2.8.2 概率计算问题
1、直接计算方法
给定模型λ=（A,B,π）和观测序列O = (o1,o2,…,oT)，计算观测序列O出现的概率P(O|λ)。最直接的方法是按概率公式直接计算。通过列举所有可能的长度为T的状态序列I = (i1, i2, …, iT)，求各个状态序列I与观测序列O = (o1,o2,…,oT)的联合概率P(O,I|λ)，然后对所有可能的状态序列求和，得到P(O|λ)。
状态序列I = （i1,i2,…,iT）的概率是
P(I|λ)=π_1 a_(i_1 i_2 ) a_(i_2 i_3 )∙∙∙a_(i_(T-1) i_T )
对固定的状态序列I=(i1,i2,…,iT)，观测序列O = (o1,o2,…,oT)的概率是P(O|I,λ),
P(O│I,λ)=b_(i_1 ) (o_1 ) b_(i_2 ) (o_2 )∙∙∙b_(i_T ) (O_T)
O和I同时出现的联合概率为
P(O,I│λ)=P(O│I,λ)P(I│λ)
                           = π_(i_1 ) b_(i_1 ) (o_1)a_(i_1 i_2 ) b_(i_2 ) (o_2)∙∙∙a_(i_(t-1) i_T ) b_(i_T ) (o_T)
然后，对所有可能的状态序列I求和，得到观测序列O出现的概率P(O|λ)，即
P(O│λ)=∑_I▒〖P(O│I,λ)P(I|λ)〗
                        = ∑_(i_1 i_2…i_T)▒〖π_(i_1 ) b_(i_1 ) (o_1)a_(i_1 i_2 ) b_(i_2 ) (o_2 〗)…a_(i_(T-1) i_T ) b_(i_T ) (o_T)
但是，利用上面公式计算量很大，是O(TNT)阶的，这种算法不可行。
下面介绍计算观测序列概率P(O|λ)的有效算法：前向-后向算法。
2、前向算法
前向概率
给定隐马尔可夫模型λ，定义到时刻t部分观测序列为o1,o2,…,ot且状态为qi的概率为前向概率，记作
a_t (i)=P(o_1,o_2,…,o_t,i_t=q_i |λ)
可以递推地求得前向概率ai(i)及观测序列概率P(O|λ).
观测序列概率的前向算法
输入：隐马尔可夫模型，观测序列O；
输出：观测序列概率P(O|λ);
（1）初值
                 a_1 (i)=π_i b_i (o_1 ),    i=1,2,…,N
（2）递推 对t = 1,2,…,T-1,
            a_(i+1) (i)=[∑_(j=1)^N▒〖a_t (j) a_ji 〗] b_i (o_(t+1) ),i=1,2,…,N
（3）终止
P(O│λ)=∑_(i=1)^N▒〖a_T (i)〗
前向算法，步骤（1）初始化前向概率，是初始化时刻的状态it = qi和观测o1的联合概率。步骤（2）是前向概率的递推公式，计算到时刻t+1部分观测序列为o1,o2,…,ot,ot-1且在时刻t+1处于状态qi的前向概率。在（2）的公式中，既然at(j)是到时刻t

3. 模型评价
3.1 模型检验
3.1.1 T检验
3.1.2 似然比检验
3.1.3 Wald检验
4. 优化问题
4.1 梯度下降法
4.2 拟牛顿法
4.
4.3 凸二次规划问题
4.3.1 概述
凸优化问题是指约束最优化问题
min┬w⁡〖f(w)〗
s.t.g_i (w)≤0,i=1,2,∙∙∙,k
h_i (w)=0,i=1,2,∙∙∙,l
其中，目标函数f(w)和约束函数gi(w)都是Rn上的连续可微的凸函数，约束函数hi(w)是Rn上的放射函数。
当目标函数f(w)是二次函数且约束函数gi(w)是放射函数是，上述凸优化问题成为凸二次规划问题。
4.4 牛顿法
4.5 共轭梯度方法
共轭梯度方法是用来解决
Qx=b
对于x属于Rn×1,Q属于Rn×n，（QT=Q，Q>0），b属于Rn+1。求解上式等价于求解标量函数
φ(x)=1/2 x^T Qx-x^T b
的最小值。在共轭梯度法中，有一个方向向量集{d0,d1,••,dn-1}与矩阵Q共轭，即diQdj=0，i !=j。在迭代过程的第k次迭代过程中，把目标函数当前计算的负梯度向量和以前方向向量线性组合生成的共轭方向向量。共轭梯度法的优点在于：
（1）用非常简单的公式确定新的方向向量
（2）这使得共轭梯度法稍微比最速下降法复杂
（3）因为方向向量是基于计算的梯度


共轭梯度法
步骤1    以任意x0∈Rn×1开始。定义初始方向向量为
d_0=-g_0=-∆_x φ(x_k ) |_(k=0)=b-Qx_0
步骤2    α_k=-(g_k^T d_k)/(d_k^T Qd_k )，其中，gk=Qxk-b
步骤3    x_(k+1)=x_k+α_k d_k
步骤4    d_(k+1)=-g_(k+1)+β_k d_k，其中β_k=(g_(k+1)^T Qd_k)/(d_k^T Qd_k )
          β_k 的另一种形式是
β_k=(g_(k+1)^T g_(k+1))/(g_k^T g_k )
步骤5    回到步骤二
    这个算法在有限步内收敛，二次式问题收敛在n步内完成。    
